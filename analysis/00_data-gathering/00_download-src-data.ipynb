{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "This script is adapted from [here](https://github.com/baicalin/GAN-WGCNA/blob/main/codes/RNASeq_pipeline/1_RNASeq_Dataset_Downloader.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/madison/Downloads/Reproducibility Study/binf-6310_gan-wgcna/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/madison/Downloads/Reproducibility Study/binf-6310_gan-wgcna/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp313-cp313-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.2.3-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.3 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import date\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_FOLDER_PATH = \"../../data/00_data-gathering\"\n",
    "os.makedirs(DATASETS_FOLDER_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(PREFIX = \"GSM\", ID_LIST = list(range(2987694, 2987923 + 1)), ATTRIBUTES = [\"ID\", \"Title\", \"self-administration\", \"challenge\"]):\n",
    "\n",
    "    dfObj = pd.DataFrame([], columns = ATTRIBUTES)\n",
    "\n",
    "    for id in tqdm(ID_LIST, ncols=134):\n",
    "        web_url = \"https://www.ncbi.nlm.nih.gov/biosample/%s\" % PREFIX + str(id)\n",
    "        r = requests.get(web_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        trs = soup.find_all(\"tr\") # tr has th and td\n",
    "        appendObj = {}\n",
    "        appendObj[\"ID\"] = PREFIX + str(id)\n",
    "        appendObj[\"Title\"] = soup.find(\"title\").text.split(\" \")[0]\n",
    "\n",
    "        for atr in ATTRIBUTES:\n",
    "\n",
    "            for tr in trs:\n",
    "\n",
    "                if tr.th.text == atr:\n",
    "                    appendObj[atr] = tr.td.text\n",
    "                    break\n",
    "\n",
    "        # NOTE from Jane: I got an error about halfway through downloading the data that said attribute 'self-administration'\n",
    "        # was not found in one of th appendObjs; I changed the line to cath this error, but we may want to check whether our\n",
    "        # data is the same as the paper's.\n",
    "        # Original line: np.array([appendObj[key] for key in ATTRIBUTES)\n",
    "        # Revised line: np.array([appendObj[key] for key in ATTRIBUTES if key in appendObj.keys()])\n",
    "        try:\n",
    "            append_df = pd.DataFrame(np.array([appendObj[key] for key in ATTRIBUTES if key in appendObj.keys()]), index = ATTRIBUTES)\n",
    "            dfObj = pd.concat([dfObj, append_df.T])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(appendObj)\n",
    "            print()\n",
    "            print(dfObj.head(5))\n",
    "\n",
    "    return(dfObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0287eeb0fe704177983901fa22d4babd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_result = download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning!\n",
    "Jane: I got a 404 for mouse ID GSM2987724"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data(download_result):\n",
    "    label_arr = []\n",
    "    for i in range(0, len(download_result)):\n",
    "        label_arr.append([item[0] for item in download_result[\"self-administration\"] ][i] + [item[0] for item in download_result[\"challenge\"] ][i])\n",
    "    label_arr\n",
    "    download_result[\"status\"] = label_arr\n",
    "\n",
    "    label_arr = []\n",
    "    for v in download_result[\"Title\"]:\n",
    "        label_arr.append(v.split(\"-\")[0])\n",
    "        \n",
    "    label_arr\n",
    "    download_result[\"region\"] = label_arr\n",
    "    download_result.to_csv(os.path.join(DATASETS_FOLDER_PATH, \"downloaded_data.csv\"), index = False)\n",
    "    return download_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_result = label_data(download_result)\n",
    "download_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(\n",
    "                        index = [\"BLA\", \"Cpu\", \"Hipp\", \"NAc\", \"PFC\", \"VTA\"],\n",
    "                        columns = [\"SS\", \"SC\", \"CS\", \"CC\", \"CN\", \"SN\"]\n",
    "                        )\n",
    "for region in [\"BLA\", \"Cpu\", \"Hipp\", \"NAc\", \"PFC\", \"VTA\"]:\n",
    "    for status in [\"SS\", \"SC\", \"CS\", \"CC\", \"CN\", \"SN\"]:    \n",
    "        df_meta.loc[region, status] = len(download_result[(download_result[\"region\"] == region) & (download_result[\"status\"] == status)])\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.to_csv(os.path.join(DATASETS_FOLDER_PATH, \"metadata.csv\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning\n",
    "Jane: We are off-by-1 here at BLA-SN (original paper has 7, we only have six because of the 404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning\n",
    "Jane: I couldn't find in the repo where \"DATASETS/original.CSV\" in post_modification_and_save_to_csv() comes from; I am taking a wild guess that it is [this data linked from the paper](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE110344) so I wrote the code below to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "def download_file(url, name):\n",
    "    out_file = DATASETS_FOLDER_PATH + name + '.tsv'\n",
    "\n",
    "    # Download archive\n",
    "    try:\n",
    "        # Read the file inside the .gz archive located at url\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            with gzip.GzipFile(fileobj=response) as uncompressed:\n",
    "                file_content = uncompressed.read()\n",
    "\n",
    "        # write to file in binary mode 'wb'\n",
    "        with open(out_file, 'wb') as f:\n",
    "            f.write(file_content)\n",
    "            return 0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 1\n",
    "    \n",
    "\n",
    "download_file('https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE110344&format=file&file=GSE110344%5Fcounttab%5Fself%5Fadministration%2Etxt%2Egz', 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original = pd.read_csv(DATASETS_FOLDER_PATH + \"original.tsv\", sep=\"\\t\")\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to download the behavioral index data from the paper's repo:\n",
    "\n",
    "behavioral_index_url = \"https://media.githubusercontent.com/media/baicalin/GAN-WGCNA/refs/heads/main/codes/RNASeq_pipeline/Datasets/Behavioral_index_data.csv\"\n",
    "\n",
    "# download and save as csv\n",
    "response = requests.get(behavioral_index_url)\n",
    "with open(os.path.join(DATASETS_FOLDER_PATH, \"behavioral_index_data.csv\"), 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Madison: Had to change downloaded_data.tsv -> downloaded_data.csv\n",
    "\n",
    "Warning: Got a key error because df is not defined \n",
    "Added df = original #Assign original DataFrame to df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_modification_and_save_to_csv(dfObj, result_id=None):\n",
    "    # If no result_id was provided, create one based on the current time.\n",
    "    if result_id is None:\n",
    "        result_id = datetime.datetime.now().strftime(\"%H%M%S\")\n",
    "    \n",
    "    # Define paths for the input files.\n",
    "    original_path = os.path.join(DATASETS_FOLDER_PATH, \"downloaded_data.tsv\")\n",
    "    behavioral_path = os.path.join(DATASETS_FOLDER_PATH, \"behavioral_index_data.csv\")\n",
    "    \n",
    "    # Read the input datasets.\n",
    "    df = pd.read_csv(original_path, sep=\"\\t\")\n",
    "    df_bi = pd.read_csv(behavioral_path)\n",
    "    \n",
    "    # Build new DataFrame columns using a dictionary to avoid iterative insertion.\n",
    "    new_columns = {}\n",
    "    for col in df.columns[1:231]:\n",
    "        # Extract metadata from dfObj.\n",
    "        # (The [2:3] slicing mimics the original behavior that extracts a character from\n",
    "        # the string representation of the numpy array. Adjust as needed.)\n",
    "        challenge = str(dfObj.loc[dfObj['Title'] == col, \"challenge\"].values)[2:3]\n",
    "        sa = str(dfObj.loc[dfObj['Title'] == col, \"self-administration\"].values)[2:3]\n",
    "        \n",
    "        # Determine region and sample number based on column name.\n",
    "        region = col[0:3]\n",
    "        sample_num = col[-3:]\n",
    "        \n",
    "        # Get the Addiction Index from the behavioral data.\n",
    "        ai = str(df_bi.loc[df_bi['Animal.ID'] == int(sample_num), \"Addiction Index\"].values[0])\n",
    "        \n",
    "        # Create a new column name (e.g., \"001_BLA_AB-3.0\").\n",
    "        new_col_name = f\"{col[-3:]}_{region.upper()}_{sa}{challenge}-{ai}\"\n",
    "        new_columns[new_col_name] = df[col].values\n",
    "\n",
    "    df = original #Assign original DataFrame to df\n",
    "    \n",
    "    # Create a new DataFrame with the new columns, using the 'Gene' column as the index.\n",
    "    df_with_denoted_colnames = pd.DataFrame(new_columns, index=df[\"Gene\"])\n",
    "    \n",
    "    # Save the complete DataFrame to CSV. The index is labeled \"Gene\" so that it is saved and\n",
    "    # can be reloaded easily.\n",
    "    output_file = os.path.join(DATASETS_FOLDER_PATH, f\"Total_Conditions_Counts_with_AI{result_id}.csv\")\n",
    "    df_with_denoted_colnames.to_csv(output_file, index_label=\"Gene\")\n",
    "    \n",
    "    # Reload the CSV file.\n",
    "    # Using index_col=\"Gene\" ensures that the gene names become the DataFrame index.\n",
    "    df_loaded = pd.read_csv(output_file, index_col=\"Gene\")\n",
    "    \n",
    "    # List of regions to process.\n",
    "    regions = [\"BLA\", \"VTA\", \"CPU\", \"HIP\", \"NAC\", \"PFC\"]\n",
    "    \n",
    "    # For each region, select the columns that belong to it and save the DataFrame.\n",
    "    for region in tqdm(regions, ncols=134, desc=\"Processing Regions\"):\n",
    "        # Identify all columns where positions 4 to 6 match the region.\n",
    "        # (e.g., in \"001_BLA_AB-3.0\", characters 4 to 7 are \"BLA\".)\n",
    "        region_cols = [col for col in df_loaded.columns if col[4:7] == region]\n",
    "        \n",
    "        # Create a DataFrame with these columns.\n",
    "        df_region = df_loaded[region_cols].copy()\n",
    "        \n",
    "        # Save the region-specific DataFrame with and without the index.\n",
    "        region_out_with_index = os.path.join(DATASETS_FOLDER_PATH, f\"{region}_Count_Dataset{result_id}.csv\")\n",
    "        region_out_without_index = os.path.join(DATASETS_FOLDER_PATH, f\"{region}_Count_Dataset_no_index{result_id}.csv\")\n",
    "        df_region.to_csv(region_out_with_index, index=True)\n",
    "        df_region.to_csv(region_out_without_index, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "post_modification_and_save_to_csv(dfObj = download_result, result_id=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
